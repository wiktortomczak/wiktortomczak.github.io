..
  explain why angle filter
  error areas
  target object picture, description
  error vs speed
  explain interpolation, why


.. TODO: plots
.. TODO: cumulative error distribution
.. TODO: plot axis labels, ticklabels
.. TODO: picture 1, picture 2
.. TODO: numbers
.. TODO: based on right sensor? right, not left?
.. TODO: answers - errors at extreme angle range??
.. TODO: video description
.. TODO: remove footnote backlinks
.. TODO: other TODOs
.. TODO: code
.. TODO: Fix TODOs
.. TODO: index.html

.. |le|  unicode:: U+2264
.. |in|  unicode:: U+220A
.. |nbsp|  unicode:: U+00A0
   
HC-SR04 distance sensor study
#############################

This is a study of `HC-SR04 ultrasonic distance sensors`__, of the data /
readings they provide and their applicability in the `sumo robot <src_>`_.

__ https://google.com/search?q=HC-SR04+ultrasonic+sensor

Goals / motivations
*******************

The purpose of this study of HC-SR04 distance sensors is to understand

* What kind of vision / perception of the robot's environment the sensors
  provide?
* How to build the robot's object detection system based on these sensors?

Questions
*********

The questions that the study aims to answer are:

1. What readings does a sensor produce?
2. What is a sensor's field of view (receptive field)?
3. | How unambiguous the readings are?
   | (What extent of object locations maps to a single reading value?)
4. | How reliable / consistent the readings are?
   | (What range of readings is produced for a fixed object location / distance?)
5. Do adjacent sensors interfere with each other?
 
Key results
***********

Answers
=======

In summary, the answers to the above questions are:

1. What readings does a sensor produce?

  Most of the time, sensor readings are the distance to the object in its field
  of view, as one would expect. The readings are fairly accurate, the correct
  distance +- TODO mm. This is consistenly so for all object distances and angles
  in the sensor's field of view (answer 2.).  TODO angle error range
  
  However, at object distance over 1m, TODO% readings become (very) inaccurate.
  This is likely due to interference between sensors, whose fields of operation
  overlap, or due to another object affecting the experiment.
  
  | See :ref:`Distance reading vs target angle distribution`.
  | See :ref:`Chart 7<chart7>`.
  
  A sensor produces a reading every TODO ms on average, which amounts to 
  TODO readings per 1 s.

2. What is a sensor's field of view (receptive field)?

  A sensor detects objects at a range of distances from 0 mm at least up to 1 m
  and possibly further (answer 1.), and at a range of angles +- TODO deg relative
  to sensor's front axis.
  
  | See :ref:`Distance reading vs target angle distribution`.
  | See :ref:`Chart 2<chart2>`.

3. How reliable / unambiguous the readings are?

  Sensor readings reliably reflect object distance, with +- TODO mm accuracy, if
  the object is up to 1m away and possibly further, and at an angle [0, +-TODO]
  deg from the sensor's front axis. If the object is at an angle +-[TODO, TODO]
  deg, TODO% readings are unreliable. The readings do not indicate object's angle,
  obviously.
  
  In others words, given a reading, the object is at the measured distance +- TODO mm,
  at an unknown angle between [TODO, TODO], if the object is sure to be within 1 m
  distance and this angle range. If the object can be at angle TODO, any isolated
  reading is unreliable.

4. How consistent the readings are?

  See :ref:`Chart 4<chart4>`.

5. Do adjacent sensors interfere with each other?

  The sensors interfere in the overlapping areas of their fields of view,
  especially in their parts further away from the sensors.
  
  | See :ref:`Chart 6<chart6>`.
  | See :ref:`Chart 7<chart7>`.

Note: Answers related to sensors's distance and angle range were based on the
data from the ``right`` sensor. It was taken as a proxy for a single isolated
sensor, as it overlapped the least with other sensors.

Conclusions
===========

Reliable detection of object's angle in addition to object's distance is only
possible if the angular field of view [1]_ is limited, thus limiting the
uncertainty about the angle.

.. [1] the range of angles at which the sensor can detect the object at all

Sensors should be placed so that their fields of operation do not overlap,
so facing directions diverging by at least TODO deg.

It could be possible to improve reading accuracy, also in areas of low accuracy
/ sensor overlap, using information from a sequence of readings from all
sensors, rather than from isolated readings without context

Study details
*************

The remaining part of this document explains the study, how the answers were
established.

Experiment
==========

Setup
-----

.. figure:: scene.png

  Experiment setup. Sensors facing an empty room with a controlled object.

.. TODO: a picture from the side.
  
In a controlled experimental session, an :ref:`array of three distance sensors
<sensors>` was placed on the floor, facing an empty room, and operated. A single
:ref:`round metal object<Sensor target>` was being moved in front of the sensors
to a number of :ref:`different locations<Target object locations>`, at varying
distance from the sensors. :ref:`Distance readings from sensors<Sensor distance
readings>` were recorded. The session was also recorded in :ref:`a video<Sensor
target video recording>`, from a camera installed above the floor, facing down.
This allowed to recover actual sensor-to-object distances and angles.

Analysis
--------

Later offine, distance readings from the sensors were confronted with distance
information :ref:`extracted from the video<Video object detection>`. Sensor
distance readings were :ref:`related<Data analysis>` to locations / distance
estimates of the metal object, acting as controlled sensor target. A number of
:ref:`charts<Charts>` displaying how the readings relate to the object's
locations were drawn and examined. The charts made apparent the :ref:`answers
<Answers>` to :ref:`study questions<Questions>`.

.. figure:: sensor_hit.png

  Correct sensor distance reading - matches target distance estimate from
  the video.

.. figure:: sensor_error.png

  Incorrect sensor distance reading - matches target distance estimate from
  the video.

.. _miss:

.. figure:: sensor_miss.png

  Sensor miss - reading value close to distance to the wall.

Sensors
-------

The three sensors had the following spatial arrangement:

.. figure:: sensors.jpg

  Spatial arrangement of sensors ``left``, ``front``, ``right``. In the image,
  labels are next to each sensor's echo piece.

The sensors were driven by a microcontroller running `sensor driver code
<src_controller_drivers_distance_sensor_h_>`_ alone. The code repeatedly fired
and read the sensors, each sensor simultaneously and independently. Readings
were streamed live to a PC, where they were saved.

Timestamps of echo high and low signals were measured with microsecond precision
and collected microseconds after they occured, in an interrupt handler.

Sensor target
-------------

TODO picture, description

Data analysis
=============

In the above :ref:`experiment<Experiment>`, two streams of complementary
sensor-related data were recorded:

1. Sensor distance readings.
2. Video frames with the sensor target's location.

To relate sensor distance readings (1) to sensor target (the metal object),
the target's locations in video frames (2) were :ref:`mapped<Mapping space and
time coordinates>` to estimates of:

3. Sensor target's locations in the sensor X-Y plane (the floor).
4. Distances and angles from each sensor's echo piece to sensor target.

Sensor-to-target distance estimates (4), taken as expected distance readings,
were compared with actual sensor distance readings (1), producing an estimate
of sensor reading error:

5. *reading error* = sensor's distance reading (1) - expected distance reading (4)

The above measure of error is based on two main assumptions:

* Good accurracy of sensor-to-target distance estimates (4).
* Good calibration of sensor distance readings (1) - that sensors readings
  indeed indicate the distance, in millimeters.
   
The relationship between sensor distance readings (1) and quantities (3, 4, 5),
were visualized on a number of :ref:`charts<Charts>` and in a :ref:`video
<Video>`, each presenting a different view of the total 3 dimensions:

* sensor distance readings [millimeters, 1D]
* sensor target's location / distance [2D], one of:
   * location [(x, y) coordinates in the sensor plane, 2D]
   * location [(distance, angle) coordinates in the sensor plane, 2D]
   * distance reading error [millimeters, 1D]

Mapping space and time coordinates
----------------------------------

Pixel coordinates to sensor plane coordinates
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

First, sensor target's video frame coordinates in pixels of were mapped to
the sensors X-Y plane (the floor) coordinates in millimeters, via an `affine
transformation <https://en.wikipedia.org/wiki/Transformation_matrix#Affine_transformations>`_
of 2D vector space.

Once the sensor target's location in the sensor plane (the floor) was known,
distance in millimeters and angle (direction) from each sensor's echo piece
to the sensor target was computed. Specifically, a (distance, angle) vector
was computed between two pairs of x-y coordinates in the sensor plane:

* The sensor's echo piece.
* The closest point in the sensor target's contour.

The closest point was determined as the closest point on a circle, the
approximate shape of the target round metal object's contour in the sensor
plane.

Video time to sensor time
^^^^^^^^^^^^^^^^^^^^^^^^^

TODO

| sensor time
| video time
| frame id
| definitions
| how measured
| how one mapped to the other

Charts
======

.. TODO: for each chart, description of what is presented, axis, scales, colors
.. TODO: explanations of answers to questions,

.. _chart1:
   
Chart 1: Target location XY vs distance reading
-----------------------------------------------

.. image:: ../analysis/TargetXYVsSensorsDistance.left.png

.. image:: ../analysis/TargetXYVsSensorsDistance.front.png

.. image:: ../analysis/TargetXYVsSensorsDistance.right.png

These charts show complete :ref:`source data<Source data>` with minimal processing.

Each spot in the plot corresponds to a single reading of the given sensor.

* The spot's X-Y coordinates are the sensor plane (the floor) location of the
  sensor target - of the object that presumably triggered the sensor
  reading (see :ref:`known problems<Known problems and limitations>` though).
* The spot's color corresponds to sensor reading value, the distance it
  measured.
* TODO concentric circles

Location of each sensor's echo piece is marked with a black point.

.. _chart2:

Chart 2: Target location XY vs distance reading, sliced
-------------------------------------------------------

.. image:: ../analysis/TargetXYVsSensorsDistance.left.sliced.gif

.. image:: ../analysis/TargetXYVsSensorsDistance.front.sliced.gif

.. image:: ../analysis/TargetXYVsSensorsDistance.right.sliced.gif

These charts present the sama data as :ref:`Chart 1<chart1>`.
However, readings are sliced / broken down into 50 mm stripes. A single stripe
only shows those target object's locations for which the reading was in the
stripe's 50 mm reading range -  presumably triggered by the target object.
Gray locations are target object's locations for which the reading value was
outside the stripe's range.

Observations:

* Distance readings roughly correspond to actual target's distance.
* Some readings are visibly incorrect: 
   * ``left`` sensor: in the [1000, ...] mm range.
   * ``front`` sensor: in the [450, 500] and [1400, 1600] mm range.
   * ``right`` sensor: in the [1150, 1200] and [1300, 1500] mm range.
* The incorrect readings are possibly due to an external cause, as they are only
  incorrect in ranges that are narrow, specific and different for each sensor.
* Sensor's angular field of view [1]_ is constant, regardless of distance and
  for all sensors.

.. _chart3:

Chart 3: Target location XY vs reading error
--------------------------------------------

.. image:: ../analysis/TargetXYVsSensorsError.left.png

.. image:: ../analysis/TargetXYVsSensorsError.front.png

.. image:: ../analysis/TargetXYVsSensorsError.right.png

These charts present the sama data as the :ref:`Chart 1<chart1>`.
However, the color indicates :ref:`reading error<Reading error color scale>`,
rather than distance. The areas in green are areas of correct readings - where
the reading values are very close to the distance to the target object. The
areas in red, conversely, are areas of incorrect readings.

Observations:

* ``right`` sensor readings, compared to ``left`` and ``center``, are correct in
  a much wider range of sensor-to-target angles, at least twice as wide. This is
  likely due to larger interference betwen the latter two sensors.
* Readings become increasingly incorrect close to the limits of the above angle
  range. These is the case for all sensors and seems unaffected by sensor
  interference, as it occurs both at outer limits (left of the ``left`` sensor,
  right of the ``right`` sensor) and at inner limits, that overlap between
  sensors.
* In the range of angles where readings are correct, they are correct at all
  distances.
* ``right`` sensor: readings are incorrect in the far right corner, for an
  unknown reason.
    
.. TODO: error range 0-100
.. TODO: bigger alpha closer to red
     
XY chart notes
--------------
In the above X-Y charts, the time dimension can be recovered by running the
:ref:`chart-generating code<Source code>` with the ``--annotate=...`` flag.

Reading error color scale
-------------------------

.. image:: reading_error.png
   :class: float-left

Where noted, the color represents :ref:`distance reading error<Data analysis>`,
that is, the difference between sensor distance reading and expected reading,
rather than simply the former. In these cases, red locations are target's
locations for which the sensor's reading of distance to target is off
(too low or too high) by 50 mm or more. Gray locations are :ref:`misses<miss>`
- target's locations that triggered an "empty" reading of the stationary wall.

.. _chart4:

Chart 4: Distance reading vs target distance
--------------------------------------------

.. image:: ../analysis/SensorsDistanceVsTargetDistance.left.png

.. image:: ../analysis/SensorsDistanceVsTargetDistance.front.png

.. image:: ../analysis/SensorsDistanceVsTargetDistance.right.png

| TODO: observations
| roughly correct
| front 500, left 1000+, front 1200+, right 1200+
| spread with angle

| TODO: legend

.. _chart5:

Chart 5: Distance reading vs cumulative reading error distribution
------------------------------------------------------------------

.. image:: ../analysis/SensorsDistanceVsCumulativeErrorDistribution.left.png

.. image:: ../analysis/SensorsDistanceVsCumulativeErrorDistribution.front.png

.. image:: ../analysis/SensorsDistanceVsCumulativeErrorDistribution.right.png

.. _chart6:

Chart 6: Distance reading vs reading error distribution
-------------------------------------------------------

.. image:: ../analysis/SensorsDistanceVsErrorDistribution.left.png

.. image:: ../analysis/SensorsDistanceVsErrorDistribution.front.png

.. image:: ../analysis/SensorsDistanceVsErrorDistribution.right.png

These charts present the distribution of distance reading error in each distance
reading interval. That is, given a reading, how likely the reading is correct
and how big the error possibly is.

TODO: How these charts relate to :ref:`Chart 4<chart4>`.

Observations:

* ``right`` sensor: Readings below ca. 1000 mm are correct and accurate,
  error |le| 50 mm.
* ``left``, ``center`` sensors: Readings below ca. 1000 mm are also quite
  correct, but less so, error |in| [25, 75] mm. This is likely due to larger
  interference betwen these two sensors.
* ``front`` sensor: Many readings of ca. 500 mm are wrong. Possibly, another
  object repeatedly entered the sensor's field of view during the experiment,
  at 500 mm distance, while the reference target was at various locations and
  distances.

.. _chart7:

Chart 7: Target angle vs reading error distribution
---------------------------------------------------

.. image:: ../analysis/TargetAngleVsErrorDistribution.left.png

.. image:: ../analysis/TargetAngleVsErrorDistribution.front.png

.. image:: ../analysis/TargetAngleVsErrorDistribution.right.png

These charts present the distribution of distance reading error in each
sensor-to-target angle interval. That is, how does the reading error depend
on the angle at which the target is located wrt. the sensor.

Observations:

* The range of sensor-to-target angles with correct readings is:
   * ``left`` and ``right`` sensors : ca. 70 deg
   * ``front`` sensor : ca. 60 deg, more narrow likely due to more interference
* ``right`` sensor: In the above angle range, readings are correct,
  error |le| 50 mm.
* ``right`` sensor: Many readings are incorrect in the [60, 70] deg interval.
  These are likely the readings when the target object was in the far right
  corner.  

Video
=====

.. youtube:: TBBezBjlw-4

The video superposes sensor readings on the :ref:`video recording of the
sensor's target object's locations<Sensor target video recording>`

The readings are visualized as:

* concentric arcs radiating from each sensor's echo piece
* glowing of each echo piece
* sliding bars with reading error 

The color of arcs (readings) and sliding bars (errors) indicates :ref:`reading
error<Reading error color scale>`.

Arc angle span (width) is set explicitly to a fixed value TODO deg, based on
:ref:`answer 2.<answers>`

The video was primarily useful for solving problems with the study itself, by
inspecting relevant video frames. It allowed eg.

* to identify experimental causes of reading error
* to double-check target object's location extracted from the video

The video corresponds to video frame ids [TODO-TODO] in the :ref:`combined
source data<Raw data combined>` dump.

Source data
===========

The source data underlying the study consisted of two streams of complementary
sensor-related data:

1. :ref:`Sensor distance readings` - ~9k readings in the overlapping time
   window [2]_.
2. :ref:`Video frames<Sensor target video recording>` with the sensor target's
   location - TODO frames in the overlapping time window [2]_.

.. [2] The overlapping time window, in which both distance readings and video frames
       were recorded, was 2 min 17 s.

See :ref:`Chart 1<chart1>`.

Target object locations
-----------------------

The target round metal object's locations were a dense sample of the sensor X-Y
plane's (the floor) area recorded in the video. Two types of the target object's
moves were recorded:

* Slow parallel sweeps, mostly orthogonal to the sensors.
* Faster random moves, mostly towards / away from the sensors.

Raw data combined
-----------------

TODO: embed DataFrame 

Sensor distance readings
------------------------

TODO 

| sensor recording content, format
| actual distance sensor reading, in millimeters

Sensor target video recording
-----------------------------

TODO

Video object detection
----------------------

TODO

| video object extraction
| frames
| interpolation

Source code
===========

`Data analysis source code <src_distance_sensors_analysis_analyze_py_>`_
- produced the :ref:`charts<Charts>`, the :ref:`video<Video>` and the
:ref:`combined source data<Raw data combined>` dump.

Known problems and limitations
******************************

* Artifact distance reading errors, likely due to experimenters' legs entering
  the sensor's field of operation
* Incorrect estimates of sensors target's location and distance, due to:
   * Errors in video object detection
      * Incorrect object location, in particular at frame right (far) edge
        (distance to sensors TODO mm)
      * Missed object in TODO frames
   * Errors in translation from camera plane to sensor plane, due to:
      * Incorrect approximation of 3D space by 2D space, based on an assumption
        that camera and sensor planes are parallel
      * Optical distortions, perspective
   * Errors in interpolation of target's location, from locations at
     video frames immediately preceding and following a reading
   * Errors in mapping video time to sensor time
   * Errors in reading times reported by sensor
* Specific shape and material of sensor target: round and metal

Future work
***********

* Remove artifact distance reading errors by fixing the experimental procedure
* Fix estimates of sensor targets' location and distance
* Analyze a single sensor, free of interferences with other sensors
* Analyze multiple sensors, arranged spatially without overlap
* Build and evaluate a complete object detection system from combined sensors
* Confirm that sensor reading frequency is as high as possible or remove the cause
  of slowdown

.. TODO: src directive

.. _src: https://github.com/wiktortomczak/sumo

.. _src_distance_sensors_analysis_analyze_py: https://github.com/wiktortomczak/sumo/distance_sensors/analysis/analyze.py

.. _src_controller_drivers_distance_sensor_h: https://github.com/wiktortomczak/sumo/controller/drivers/distance_sensor.h
